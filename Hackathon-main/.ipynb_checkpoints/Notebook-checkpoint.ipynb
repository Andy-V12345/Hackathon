{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4ef7764a",
   "metadata": {},
   "source": [
    "# Problem Statement: Using generative AI for sustainability - How can AI be used for decarbonization.\n",
    "\n",
    "Contributors: Andy Vu, Seshu Mallina, Govardhan\n",
    "\n",
    "Project Leader: Andy Vu\n",
    "\n",
    "Team Name: SAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9d36dc",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Motivation for participating in decarbonization stems from a deep understanding of the urgent need to address the escalating environmental challenges posed by climate change. Decarbonization refers to the process of reducing carbon dioxide and other greenhouse gas emissions in order to mitigate the effects of climate change. It involves transitioning from fossil fuels to cleaner and renewable energy sources, implementing energy-efficient technologies, and adopting sustainable practices across various sectors of society.\n",
    "\n",
    " \n",
    "\n",
    "The motivation to engage in decarbonization arises from a recognition of the detrimental impacts of climate change on our planet and the well-being of future generations. Rising global temperatures, extreme weather events, sea-level rise, and ecological disruptions are just a few examples of the consequences of unchecked carbon emissions. By actively participating in decarbonization efforts, individuals, communities, businesses, and governments aim to mitigate these risks, preserve the natural environment, and create a sustainable future.\n",
    "\n",
    " \n",
    "\n",
    "Decarbonization is vital in our society for several reasons. First and foremost, it is crucial for averting the worst effects of climate change and safeguarding the planet's habitability. By reducing greenhouse gas emissions, we can limit global warming, protect ecosystems, and promote biodiversity. Additionally, decarbonization brings numerous societal benefits, including improved air quality, enhanced public health, and reduced dependency on finite fossil fuel resources. It also drives innovation and economic opportunities, as the transition to clean energy and sustainable practices creates jobs and fosters the development of new technologies and industries. Furthermore, decarbonization helps build resilient communities and promotes social equity by ensuring access to affordable and clean energy for all, regardless of socioeconomic status or geographical location.\n",
    "\n",
    " \n",
    "\n",
    "In summary, the motivation to participate in decarbonization arises from a shared responsibility to combat climate change and secure a sustainable future. By reducing carbon emissions and embracing cleaner alternatives, we can protect the environment, improve public health, drive economic growth, and foster a more equitable society. Decarbonization is not only important for addressing the challenges we face today but also for creating a better world for generations to come."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec2c470",
   "metadata": {},
   "source": [
    "# Our Solution\n",
    "Infosys, a leading global technology services and consulting company, has been at the forefront of sustainability and decarbonization efforts. They have a strong commitment to environmental responsibility and have set ambitious goals to become carbon neutral. Infosys has implemented various initiatives to reduce their carbon footprint, including energy-efficient infrastructure, renewable energy adoption, and waste management programs. They have also actively pursued green building certifications and invested in innovative technologies to optimize energy usage across their operations.\n",
    "\n",
    " \n",
    "\n",
    "Our solution, the AI-powered chatbot for energy, cleantech, and sustainability, aligns perfectly with Infosys' goals and vision for becoming carbon neutral. The chatbot's predictive models, optimization module, and intelligent recommendations would provide invaluable support to Infosys in their journey towards decarbonization. By leveraging historical and real-time data, the chatbot can identify peak energy usage patterns, optimize the utilization of renewable energy sources, and suggest energy-saving practices for Infosys' facilities.\n",
    "\n",
    " \n",
    "\n",
    "The chatbot's ability to determine optimal locations for EV charging stations would also be highly beneficial for Infosys. By strategically placing charging infrastructure based on traffic patterns and population density, Infosys can encourage the adoption of electric vehicles among its employees and visitors, further reducing carbon emissions from transportation.\n",
    "\n",
    " \n",
    "\n",
    "Moreover, the chatbot's optimization module, which showcases the ideal energy mix at different times, would empower Infosys to make informed decisions regarding resource allocation and energy distribution. It would assist in maximizing the use of renewable energy sources while minimizing reliance on fossil fuels, resulting in a significant reduction in greenhouse gas emissions.\n",
    "\n",
    " \n",
    "\n",
    "Overall, our solution would complement Infosys' ongoing sustainability efforts and provide a powerful tool to accelerate their journey towards carbon neutrality. By leveraging the chatbot's insights and recommendations, Infosys can optimize their energy usage, further increase the adoption of renewable energy, promote sustainable practices, and drive a culture of environmental responsibility within their organization. The solution would not only assist Infosys in achieving their decarbonization goals but also position them as a leader in sustainable technology and inspire others to follow suit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532c222a",
   "metadata": {},
   "source": [
    "# First Checkpoint: Energy mix and grid flexibility requirements\n",
    "\n",
    "Our proposed solution effectively addresses the challenge of energy mix and grid flexibility requirements. Through the chatbot's predictive models and optimization module, we can analyze historical and real-time data to understand energy demand patterns and identify the optimal energy mix for different time intervals. By leveraging the dataset from PJM Interconnection LLC, which includes over 10 years of hourly energy consumption data, we can train our models to accurately predict energy demand in the region served by PJM.\n",
    "\n",
    " \n",
    "\n",
    "The predictive model will utilize advanced algorithms to analyze historical energy consumption patterns, taking into account factors such as time of day, day of the week, seasonal variations, and specific events that impact energy usage. This analysis allows the model to make accurate predictions about when energy demand is expected to be high or low. By incorporating this information into the chatbot, users can receive real-time recommendations on how to manage energy consumption during periods of high demand, promoting energy efficiency and reducing strain on the grid.\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "In conclusion, our solution includes a predictive model trained on PJM's dataset, which enables accurate predictions of energy demand. By leveraging this information, along with the chatbot's optimization capabilities, we can effectively address the energy mix and grid flexibility requirements. This empowers users to make informed decisions, optimize energy usage, and contribute to the decarbonization goals of the PJM region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90afdc6a",
   "metadata": {},
   "source": [
    "[Link to Dataset](https://www.kaggle.com/datasets/selfishgene/historical-hourly-weather-data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274d6475",
   "metadata": {},
   "source": [
    "Import libraries for model we are going to make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "82612dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84bfe03",
   "metadata": {},
   "source": [
    "Read in data and combine into 1 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c305795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data_check1/AEP_hourly.csv', index_col='Datetime')\n",
    "df2 = pd.read_csv('data_check1/DAYTON_hourly.csv', index_col='Datetime')\n",
    "df3 = pd.read_csv('data_check1/PJME_hourly.csv', index_col='Datetime')\n",
    "df4 = pd.read_csv('data_check1/PJMW_hourly.csv', index_col='Datetime')\n",
    "\n",
    "# For visualization purpose\n",
    "df_final = df1.join([df2, df3, df4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e891e",
   "metadata": {},
   "source": [
    "We want to know basic information ie column headers and null counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3d3322f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d814bfa2",
   "metadata": {},
   "source": [
    "We want to find the main stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3c016ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2672ef4e",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "We are plotting the data to see trends or observations ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "170eeac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.index = pd.to_datetime(df_final.index)\n",
    "\n",
    "df_final.plot(figsize=(20,8))\n",
    "plt.title('PJM Energy Cosumption', weight='bold', fontsize=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4521fa5",
   "metadata": {},
   "source": [
    "We want to see which hours are the ones that are high demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fc36cf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Hour'] = df_final.index.hour\n",
    "df_final['Day'] = df_final.index.day\n",
    "df_final['Month'] = df_final.index.month\n",
    "df_final['Quarter'] = df_final.index.quarter\n",
    "df_final['Year'] = df_final.index.year\n",
    "\n",
    "columns = ['AEP_MW', 'DAYTON_MW', 'PJME_MW', 'PJMW_MW']\n",
    "\n",
    "f, axes = plt.subplots(nrows=6, ncols=2, figsize=(20, 14))\n",
    "f.suptitle('Daily Average Energy Consumption', weight='bold', fontsize=25)\n",
    "# We just need 11 figures, so we delete the last one\n",
    "f.delaxes(axes[5][1])\n",
    "\n",
    "for i, col in enumerate(columns):\n",
    "    sns.boxplot(data=df_final, x='Hour', y=col, ax=axes.flatten()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bdc050ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things todo: edit the loop such that it shows every year for each state/city\n",
    "f, axes = plt.subplots(nrows=6, ncols=2, figsize=(20, 14))\n",
    "f.suptitle('Monthly Average Energy Consumption', weight='bold', fontsize=25)\n",
    "# We just need 11 figures, so we delete the last one\n",
    "f.delaxes(axes[5][1])\n",
    "\n",
    "for i, col in enumerate(columns):\n",
    "    sns.stripplot(data=df_final, x='Day', y=col, ax=axes.flatten()[i],hue = \"Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c2515916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things todo: edit the loop such that it shows every year for each state/city \n",
    "f, axes = plt.subplots(nrows=6, ncols=2, figsize=(20, 14))\n",
    "f.suptitle('Yearly Average Energy Consumption', weight='bold', fontsize=25)\n",
    "# We just need 11 figures, so we delete the last one\n",
    "f.delaxes(axes[5][1])\n",
    "\n",
    "for i, col in enumerate(columns):\n",
    "    sns.barplot(data=df_final, x='Month', y=col, ax=axes.flatten()[i], hue = \"Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cfab633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(nrows=6, ncols=2, figsize=(20, 14))\n",
    "f.suptitle('Average Energy Consumption\\nfrom 2014-2018', weight='bold', fontsize=25)\n",
    "# We just need 11 figures, so we delete the last one\n",
    "f.delaxes(axes[5][1])\n",
    "\n",
    "for i, col in enumerate(columns):\n",
    "    sns.barplot(data=df_final, x='Year', y=col, ax=axes.flatten()[i], hue=\"Year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5b7455",
   "metadata": {},
   "source": [
    "Some observations that we made were \n",
    "\n",
    "* x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f754b26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.index = pd.to_datetime(df1.index)\n",
    "df1 = df1.sort_index()\n",
    "df2.index = pd.to_datetime(df2.index)\n",
    "df2 = df2.sort_index()\n",
    "df3.index = pd.to_datetime(df3.index)\n",
    "df3 = df3.sort_index()\n",
    "df4.index = pd.to_datetime(df4.index)\n",
    "df4 = df4.sort_index()\n",
    "df1.head()\n",
    "df2.head()\n",
    "df3.head()\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "be8f5019",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_train, df2_test = df2[df2.index < '2016-01-01'], df2[df2.index >= '2016-01-01']\n",
    "\n",
    "print('Train:\\t', len(df2_train))\n",
    "print('Test:\\t', len(df2_test))\n",
    "\n",
    "df1_train, df1_test = df1[df1.index < '2016-01-01'], df1[df1.index >= '2016-01-01']\n",
    "\n",
    "print('Train:\\t', len(df1_train))\n",
    "print('Test:\\t', len(df1_test))\n",
    "\n",
    "df3_train, df3_test = df3[df3.index < '2016-01-01'], df3[df3.index >= '2016-01-01']\n",
    "\n",
    "print('Train:\\t', len(df3_train))\n",
    "print('Test:\\t', len(df3_test))\n",
    "\n",
    "df4_train, df4_test = df4[df4.index < '2016-01-01'], df4[df4.index >= '2016-01-01']\n",
    "\n",
    "print('Train:\\t', len(df4_train))\n",
    "print('Test:\\t', len(df4_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "54075c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "df2_train['DAYTON_MW'].plot(label='Training Set')\n",
    "df2_test['DAYTON_MW'].plot(label='Test Set')\n",
    "plt.axvline('2016-01-01', color='black', ls='--', lw=3)\n",
    "plt.text('2016-02-01', 3700, 'Split', fontsize=20, fontweight='bold')\n",
    "plt.title('Data Splitting', weight='bold', fontsize=25)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "39f6e550",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_train['AEP_MW'].plot(label='Training Set')\n",
    "df1_test['AEP_MW'].plot(label='Test Set')\n",
    "plt.axvline('2016-01-01', color='black', ls='--', lw=3)\n",
    "plt.text('2016-02-01', 3700, 'Split', fontsize=20, fontweight='bold')\n",
    "plt.title('Data Splitting', weight='bold', fontsize=25)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "09074a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_train['PJME_MW'].plot(label='Training Set')\n",
    "df3_test['PJME_MW'].plot(label='Test Set')\n",
    "plt.axvline('2016-01-01', color='black', ls='--', lw=3)\n",
    "plt.text('2016-02-01', 3700, 'Split', fontsize=20, fontweight='bold')\n",
    "plt.title('Data Splitting', weight='bold', fontsize=25)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bcdd1178",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_train['PJMW_MW'].plot(label='Training Set')\n",
    "df4_test['PJMW_MW'].plot(label='Test Set')\n",
    "plt.axvline('2016-01-01', color='black', ls='--', lw=3)\n",
    "plt.text('2016-02-01', 3700, 'Split', fontsize=20, fontweight='bold')\n",
    "plt.title('Data Splitting', weight='bold', fontsize=25)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "17fba323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE\n",
    "\n",
    "dataset = tf.expand_dims(df2_train['DAYTON_MW'].head(10), axis=-1)\n",
    "\n",
    "# Generate a tf dataset with 10 elements (i.e. numbers 0 to 9)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(dataset)\n",
    "\n",
    "# Window the data but only take those with the specified size\n",
    "dataset = dataset.window(5, shift=1, drop_remainder=True)\n",
    "\n",
    "# Flatten the windows by putting its elements in a single batch\n",
    "dataset = dataset.flat_map(lambda window: window.batch(5))\n",
    "\n",
    "# Create tuples with features (first four elements of the window) and labels (last element)\n",
    "dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n",
    "\n",
    "# dataset2 = tf.expand_dims(df_train['PJME_MW'].head(10), axis=-1)\n",
    "\n",
    "# # Generate a tf dataset with 10 elements (i.e. numbers 0 to 9)\n",
    "# dataset2 = tf.data.Dataset.from_tensor_slices(dataset2)\n",
    "\n",
    "# # Window the data but only take those with the specified size\n",
    "# dataset2 = dataset2.window(5, shift=1, drop_remainder=True)\n",
    "\n",
    "# # Flatten the windows by putting its elements in a single batch\n",
    "# dataset2 = dataset2.flat_map(lambda window: window.batch(5))\n",
    "\n",
    "# # Create tuples with features (first four elements of the window) and labels (last element)\n",
    "# dataset2 = dataset2.map(lambda window: (window[:-1], window[-1]))\n",
    "\n",
    "# dataset3 = tf.expand_dims(df_train['PJMW_MW'].head(10), axis=-1)\n",
    "\n",
    "# # Generate a tf dataset with 10 elements (i.e. numbers 0 to 9)\n",
    "# dataset3 = tf.data.Dataset.from_tensor_slices(dataset3)\n",
    "\n",
    "# # Window the data but only take those with the specified size\n",
    "# dataset3 = dataset3.window(5, shift=1, drop_remainder=True)\n",
    "\n",
    "# # Flatten the windows by putting its elements in a single batch\n",
    "# dataset3 = dataset3.flat_map(lambda window: window.batch(5))\n",
    "\n",
    "# # Create tuples with features (first four elements of the window) and labels (last element)\n",
    "# dataset3 = dataset3.map(lambda window: (window[:-1], window[-1]))\n",
    "\n",
    "# Print the results\n",
    "for x,y in dataset:\n",
    "    print(\"x = \", x.numpy())\n",
    "    print(\"y = \", y.numpy())\n",
    "    print()\n",
    "    \n",
    "# dataset1 = tf.expand_dims(df_train['AEP_MW'].head(10), axis=-1)\n",
    "\n",
    "# # Generate a tf dataset with 10 elements (i.e. numbers 0 to 9)\n",
    "# dataset1 = tf.data.Dataset.from_tensor_slices(dataset1)\n",
    "\n",
    "# # Window the data but only take those with the specified size\n",
    "# dataset1 = dataset1.window(5, shift=1, drop_remainder=True)\n",
    "\n",
    "# # Flatten the windows by putting its elements in a single batch\n",
    "# dataset1 = dataset1.flat_map(lambda window: window.batch(5))\n",
    "\n",
    "# # Create tuples with features (first four elements of the window) and labels (last element)\n",
    "# dataset1 = dataset1.map(lambda window: (window[:-1], window[-1]))\n",
    "\n",
    "# # Print the results\n",
    "# for x,y in dataset1:\n",
    "#     print(\"x = \", x.numpy())\n",
    "#     print(\"y = \", y.numpy())\n",
    "#     print()\n",
    "    \n",
    "# # Print the results\n",
    "# for x,y in dataset2:\n",
    "#     print(\"x = \", x.numpy())\n",
    "#     print(\"y = \", y.numpy())\n",
    "#     print()\n",
    "    \n",
    "# # Print the results\n",
    "# for x,y in dataset3:\n",
    "#     print(\"x = \", x.numpy())\n",
    "#     print(\"y = \", y.numpy())\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5a956be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowing(data, window_size, shuffle_buffer, batch_size):\n",
    "    dataset = tf.expand_dims(data, axis=-1)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(dataset)\n",
    "    dataset = dataset.window(window_size+1, shift=1, drop_remainder=True) # window size = 24 + 1 (test)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size+1))\n",
    "    dataset = dataset.map(lambda window: (window[:-1], window[-1])) # (train, test) \n",
    "    dataset = dataset.shuffle(shuffle_buffer)\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "39abfce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = windowing(df2_train['DAYTON_MW'], 24, 72, 32)\n",
    "test = windowing(df2_test['DAYTON_MW'], 24, 72, 32)\n",
    "# train1 = windowing(df_train['PJME_MW'], 24, 72, 32)\n",
    "# test1 = windowing(df_test['PJME_MW'], 24, 72, 32)\n",
    "# train2 = windowing(df_train['AEP_MW'], 24, 72, 32)\n",
    "# test2 = windowing(df_test['AEP_MW'], 24, 72, 32)\n",
    "# train3 = windowing(df_train['PJME_MW'], 24, 72, 32)\n",
    "# test3 = windowing(df_test['PJME_MW'], 24, 72, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cd600e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=16, kernel_size=3,\n",
    "                      strides=1, padding=\"causal\",\n",
    "                      activation=\"relu\",\n",
    "                      input_shape=[24,1]),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, activation='relu')),\n",
    "    tf.keras.layers.Dense(16),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "dnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3e1f77f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model.compile(loss='mae', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "dnn_model.fit(train, validation_data=test, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "84b60b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = pd.DataFrame(dnn_model.history.history)\n",
    "metric.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7fb888b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 24\n",
    "forecast = []\n",
    "\n",
    "train_length = len(df2_train)\n",
    "forecast_series = df2[train_length - window_size:]\n",
    "\n",
    "# Use the model to predict data points per window size\n",
    "for time in range(len(forecast_series) - window_size):\n",
    "    forecast.append(dnn_model.predict(np.expand_dims(forecast_series[time:time + window_size], axis=-1)[np.newaxis]))\n",
    "\n",
    "# Convert to a numpy array and drop single dimensional axes\n",
    "results = np.array(forecast).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5db094aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_test['Pred'] = results\n",
    "\n",
    "mae = round(mean_absolute_error(df2_test['DAYTON_MW'], df2_test['Pred']), 3)\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "df2_test['DAYTON_MW'].plot(label='Actual')\n",
    "df2_test['Pred'].plot(label='Predicted')\n",
    "plt.text(16770, 3250, 'MAE: {}'.format(mae), fontsize=20, color='red')\n",
    "plt.title('Testing Set Forecast', weight='bold', fontsize=25)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfccd29",
   "metadata": {},
   "source": [
    "# Second Checkpoint: EV charging and infrastructure\n",
    "\n",
    "Our proposed solution effectively tackles the challenge of EV charging infrastructure by leveraging AI algorithms and data analysis techniques. To address this issue, our solution incorporates geographical data such as traffic patterns and population density. By acquiring and analyzing this data, we can gain valuable insights into areas with high traffic volume and concentrated population, which are prime locations for EV charging stations.\n",
    "\n",
    " \n",
    "\n",
    "Using AI algorithms, we can optimize the placement of EV chargers based on the analyzed data. The algorithms consider factors such as traffic congestion, commuting patterns, and population distribution to identify optimal locations for charging stations. By strategically placing chargers in areas where EV demand is high and where drivers frequently commute, we can ensure convenient access to charging infrastructure and promote the adoption of electric vehicles.\n",
    "\n",
    " \n",
    "\n",
    "Furthermore, the chatbot's interactive interface allows users to input specific preferences or requirements for EV charging, such as proximity to certain landmarks or availability of fast charging options. The chatbot can then generate customized recommendations that align with the user's preferences and optimize the overall charging infrastructure network.\n",
    "\n",
    " \n",
    "\n",
    "In conclusion, our solution effectively addresses the challenge of EV charging and infrastructure by leveraging AI algorithms and analyzing geographical data. By optimizing the placement of EV chargers based on traffic patterns and population density, we can ensure that charging infrastructure is strategically located to cater to the needs of EV owners. This not only facilitates the widespread adoption of electric vehicles but also contributes to the decarbonization goals of reducing greenhouse gas emissions from the transportation sector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3261e83d",
   "metadata": {},
   "source": [
    "# Third Checkpoint: The need for longer-term backup capacity\n",
    "Our proposed solution effectively addresses the need for longer-term backup capacity by leveraging AI models and predictive analysis. To tackle this challenge, we have developed an AI model that takes into account historical data from two solar plants in India spanning the last 34 days. This data includes power generation patterns, solar irradiance levels, weather conditions, and other relevant factors.\n",
    "\n",
    " \n",
    "\n",
    "By training the AI model on this dataset, we can accurately predict future power generation from the solar plants. This enables us to anticipate periods of high or low solar energy availability, which is crucial for planning backup capacity. For instance, if the AI model predicts a period of low solar power generation due to cloudy weather or other factors, it alerts the chatbot and prompts recommendations for alternative energy sources or backup systems to be activated during that period.\n",
    "\n",
    " \n",
    "\n",
    "The chatbot interacts with users in real-time, providing information and suggestions based on the AI model's predictions. It can suggest the activation of backup systems or advise on energy conservation measures during periods of reduced solar power generation. By incorporating these predictions and recommendations into the chatbot's functionality, we enable electrical companies to plan for and manage longer-term backup capacity effectively.\n",
    "\n",
    " \n",
    "\n",
    "In conclusion, our solution addresses the need for longer-term backup capacity by utilizing AI models and predictive analysis. By training the AI model on data from solar plants in India, we can accurately forecast power generation in the future. This information allows the chatbot to provide timely recommendations and assist electrical companies in planning backup capacity during periods of reduced solar energy availability. With our solution, companies can optimize their backup systems, ensure uninterrupted power supply, and contribute to a more reliable and sustainable energy infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d689fad7",
   "metadata": {},
   "source": [
    "[Link to Dataset](https://www.kaggle.com/datasets/anikannal/solar-power-generation-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4090dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import perf_counter\n",
    "from sys import getsizeof\n",
    "\n",
    "# import for improving a colorbar\n",
    "from matplotlib.colors import rgb2hex, Normalize;\n",
    "from matplotlib import rcParams\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler;\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_validate;\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Ridge;\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR, LinearSVR;\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import mean_squared_error, r2_score;\n",
    "\n",
    "# Statistical aids\n",
    "from scipy.stats import kurtosis, skew;\n",
    "\n",
    "# initialize settings\n",
    "from cycler import cycler\n",
    "rcParams['axes.prop_cycle'] = cycler(\n",
    "                                color=['navy','orange','k','b',\n",
    "                                       'y','pink', 'magenta','cyan',\n",
    "                                       'r','midnightblue',]\n",
    "                                    )\n",
    "\n",
    "# for ploting residuals distribution\n",
    "from ipywidgets import widgets, interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0bebeee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prod_p1 is for: Production of plant #1 \n",
    "prod_p1 = pd.read_csv('solar_data/Plant_1_Generation_Data.csv')\n",
    "prod_p1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "35c7fca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_p1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a7638518",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_p1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3361288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of all non zero values\n",
    "sample_df = prod_p1.loc[:, prod_p1.columns != 'PLANT_ID']\n",
    "sample_df = sample_df[(prod_p1['DAILY_YIELD'] > 0) & (prod_p1['AC_POWER'] > 0)]\n",
    "sample_df.hist(figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077f8c22",
   "metadata": {},
   "source": [
    "## Observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f8b1f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_formats(df):\n",
    "    \n",
    "    if df.columns[0].isupper():\n",
    "        initial_size = getsizeof(df)\n",
    "        supp_data = dict({'plant_id':0,'source_key':[]})\n",
    "        \n",
    "        # change column names to lowercase\n",
    "        lower_case = lambda date: date.lower()\n",
    "        df.columns = map(lower_case,df.columns)\n",
    "\n",
    "        # encode \"source_key\" into integers, store riginal \"source_key\" in separate variable  \n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(np.unique(df['source_key']))\n",
    "        df['source_key']= np.array(encoder.transform(df['source_key'].values),dtype=np.int8)\n",
    "        supp_data['source_key'] = encoder.classes_\n",
    "\n",
    "        # delete \"plant_id\" column and stores it's value in an external variable\"\n",
    "        plant_id = df['plant_id'].values[0]\n",
    "        df.drop(columns=['plant_id'],inplace=True)\n",
    "        supp_data['plant_id'] = plant_id\n",
    "\n",
    "        # change 'date_time' from string to pd.Timestamp\n",
    "        df['date_time'] =pd.to_datetime(df['date_time'].values, dayfirst=True)\n",
    "        final_size = getsizeof(df) + getsizeof(supp_data)\n",
    "        print(f'Initial size: {initial_size/1e6:.2f} Mb')\n",
    "        print(f'Final size:    {final_size/1e6:.2f} Mb')\n",
    "        print(f\"Memory footprint reduction: {(initial_size - final_size)/initial_size*100:.2f}%\")\n",
    "    else:\n",
    "        raise ValueError(\"Formats allready optimized !\")\n",
    "    return df, supp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4cfe9c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_p1, prod_p1_supp_data = optimize_formats(prod_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "455a81f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_p1_supp_data['source_key'],prod_p1_supp_data['plant_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3c43c89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_p1_dc_power = prod_p1[prod_p1['dc_power'] > 0]['dc_power'].values\n",
    "prod_p1_ac_power = prod_p1[prod_p1['ac_power'] > 0]['ac_power'].values\n",
    "\n",
    "prod_p2 = pd.read_csv('solar_data/Plant_2_Generation_Data.csv')\n",
    "prod_p2_dc_power =prod_p2[prod_p2['DC_POWER'] > 0]['DC_POWER'].values\n",
    "prod_p2_ac_power =prod_p2[prod_p2['AC_POWER'] > 0]['AC_POWER'].values\n",
    "\n",
    "data = [prod_p1_dc_power,prod_p1_ac_power,prod_p2_dc_power,prod_p2_ac_power]\n",
    "labels = ['Plant 1: DC_POWER','Plant 1: AC_POWER',\n",
    "            'Plant 2: DC_POWER','Plant 2: AC_POWER']\n",
    "plt.figure(figsize=(10,3))\n",
    "patches = plt.boxplot(data ,labels=labels,vert=False, patch_artist=True)\n",
    "patches['boxes'][0].set_facecolor( '#FF7722')\n",
    "plt.xticks([0,np.median(prod_p1_ac_power),prod_p1_ac_power.max(),\n",
    "            np.median(prod_p1_dc_power),prod_p1_dc_power.max()])\n",
    "plt.title(\"Scale Comparison AC & DC Power for the Two plants\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fe763b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plant2_eff = 100*np.max(prod_p2_ac_power)/np.max( prod_p2_dc_power)\n",
    "print(f\"Power ratio AC/DC (Efficiency) plant #2: {plant2_eff:0.3f}%\")\n",
    "plant1_eff = 100*np.max(prod_p1_ac_power)/np.max(prod_p1_dc_power )\n",
    "print(f\"Power ratio AC/DC (Efficiency) plant #1:  {plant1_eff:0.3f}%\")\n",
    "print(f\"Eff_plant_1/Eff_plant_2 (using max values): {plant2_eff/plant1_eff:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f6ce8656",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficiency = ((prod_p2_ac_power)/np.mean(prod_p2_dc_power))/(np.mean(prod_p1_ac_power)/np.mean(prod_p1_dc_power))\n",
    "print(f\"Scale ratio comparison ( using mean values ): {efficiency.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1b34031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescaling dc_power of plant #1\n",
    "def scale_dc_power(df):\n",
    "    df['dc_power'] = df['dc_power'].values/10\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "746fc5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_p1 = scale_dc_power(prod_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e023b944",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_p1_dc_power = prod_p1['dc_power'][prod_p1['dc_power']>0].values\n",
    "data2 = [prod_p1_dc_power,prod_p1_ac_power,prod_p2_dc_power,prod_p2_ac_power]\n",
    "plt.figure(figsize=(6,3))\n",
    "patches = plt.boxplot(data2,showfliers=False, vert=False,labels=labels, patch_artist=True)\n",
    "median_2 = np.median(np.hstack(data2))\n",
    "max_val_2 = prod_p1_dc_power.max()\n",
    "plt.xticks([0,median_2,max_val_2])\n",
    "plt.title(\"Corrected scale AC & DC Power for the Two plants\")\n",
    "# fig.tight_layout(pad=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "35efa7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inverter(df, supp_data, key_n, start=0, end=3400):\n",
    "    '''\n",
    "    Visualizes the ac_power, dc_power, daily_yield and total_yield of\n",
    "    an inverter in a given datetime interval\n",
    "    \n",
    "    returns: List of pyplot.axes for the 4 variables.\n",
    "    \n",
    "    Variables\n",
    "    ---------\n",
    "    df   : Pandas DataFrame with the production data\n",
    "    key_n: int from 0 to 21 representing the inverter number\n",
    "    start: int (0-3400) representing the start datetime\n",
    "    end  : int (0-3400) representing the end datetime\n",
    "    '''\n",
    "    \n",
    "    df = df.copy()\n",
    "    fig_size = (12,12)\n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    for i,item in enumerate(['ac_power','dc_power','daily_yield',\n",
    "                             'total_yield']):\n",
    "        xsize,ysize = fig_size\n",
    "        key_data = df[df['source_key'] == key_n].iloc[start:end]\n",
    "        plt.subplot(4,1,i+1)\n",
    "        plt.plot(key_data['date_time'].values, key_data[item].values,\n",
    "                 linewidth=1.5,alpha=.4)\n",
    "        ymin = key_data[item].values.min()\n",
    "        ymax = key_data[item].values.max() \n",
    "        plt.yticks(np.linspace(ymin, ymax, 5))\n",
    "#       plt.xticks(key_data['date_time'],key_data['date_time'],rotation=90)\n",
    "        plt.xticks([]) # plotting the xlabels takes too much time !!!\n",
    "\n",
    "        key  = supp_data['source_key'][key_n]\n",
    "        start_date= pd.to_datetime(key_data['date_time'].iloc[0]).date()\n",
    "        end_date  = pd.to_datetime(key_data['date_time'].iloc[-1]).date()\n",
    "        text=f'{item}, inverter #{key_n} ({key})\\nfrom {start_date} to {end_date}'\n",
    "        plt.title(text, fontsize=12)\n",
    "    fig.subplots_adjust(hspace=0.3)\n",
    "#     plt.savefig(f'inverter_{key}.png')\n",
    "    return fig.axes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ad95f06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a sample taken from one of the inverters\n",
    "# in the arrows dict xy are pints as % of x and y axis\n",
    "# in xy are 4 matrices, one for each plot 'ac_power', 'dc_power',\n",
    "#         'daily_yield' & 'total_yield'\n",
    "\n",
    "arrows =   dict({\n",
    "            'saffron_arrows':\n",
    "            dict({'color':'#FF7722','angle':140,\n",
    "                  'xy': [[[.280,.1]],\n",
    "                         [[.280,.1]],\n",
    "                         [[.295,.1],[.785,.05]],\n",
    "                         []]}),\n",
    "            'green_arrows':\n",
    "            dict({'color':'g','angle':15,\n",
    "                  'xy':[[],\n",
    "                        [],\n",
    "                        [[.39,.79],[.64,.85],[.7,.73],[.82,.88]],\n",
    "                        []]})\n",
    "                })\n",
    "\n",
    "def annotate_arrows(axes,arrows):\n",
    "    for j,ax in enumerate(axes):\n",
    "        for i,arrow in enumerate(arrows.values()):\n",
    "            for xy_raw in arrow['xy'][j]:\n",
    "                rad = lambda angle: angle/360*2*np.pi\n",
    "                xmin,xmax = ax.get_xlim()\n",
    "                xspan = xmax-xmin\n",
    "                ymin,ymax = ax.get_ylim()\n",
    "                yspan = ymax-ymin\n",
    "                xy_new = np.multiply(xy_raw,[xspan,yspan])+np.array([xmin,ymin])\n",
    "                xy_text= np.array(xy_new)+ np.array([\n",
    "                                    0.041*np.cos(rad(arrow['angle']))*xspan,\n",
    "                                    0.250*np.sin(rad(arrow['angle']))*yspan])\n",
    "                ax.annotate(\"\",xy=xy_new,xytext=xy_text,\n",
    "                            arrowprops=dict(arrowstyle=\"->\",mutation_aspect=1.2,\n",
    "                            mutation_scale=15,color=arrow['color'],lw=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "256f5209",
   "metadata": {},
   "outputs": [],
   "source": [
    "#values for key_n= 14, start=200, end=1500\n",
    "axes = plot_inverter(prod_p1,prod_p1_supp_data,14,start=200,end=1500)\n",
    "annotate_arrows(axes, arrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b766b502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_ac_power(prod_df):\n",
    "    '''this function plot all values for ac_power to visualize\n",
    "    at which time the solar power plant begins and ends production\n",
    "    to be able to separate day from night'''\n",
    "    \n",
    "    date_time = pd.DatetimeIndex(prod_df['date_time'].values)\n",
    "    xlabels = np.unique(date_time.strftime(\"%H:%M\"))\n",
    "    date_time = date_time.hour + date_time.minute/60\n",
    "    xticks =  np.unique(date_time)\n",
    "    ac_power = prod_df['ac_power'].values\n",
    "    plt.figure(figsize=(11,5))\n",
    "    ax = plt.gca()\n",
    "    ax.scatter(date_time,ac_power,s=1)\n",
    "    ax.set_xticks(xticks)\n",
    "    ax.set_xticklabels(xlabels,rotation=90,fontsize=11)\n",
    "    #not necesary to show all zero values !\n",
    "    xmin, xmax = ax.get_xlim()\n",
    "    xmin = xmin + (xmax-xmin)*0.22\n",
    "    xmax = xmax - (xmax-xmin)*0.26\n",
    "    plt.gca().set_xlim((xmin,xmax))\n",
    "    plt.title(\"ac_power by hour of the day for all inverters\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "63dc6b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_ac_power(prod_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a2eb8605",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time = pd.DatetimeIndex(prod_p1['date_time'].values)\n",
    "date_time = date_time.minute/60 + date_time.hour\n",
    "get_hr = lambda s : int(s.split(\":\")[0])+int(s.split(\":\")[1])/60\n",
    "for s in [\"05:45\",\"06:00\",\"18:30\",\"18:45\"]:\n",
    "    answer = (prod_p1[date_time==get_hr(s)]['ac_power']>0).any()\n",
    "    print(f\"Is there any ac_power > 0 at {s} ?\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8cd703ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize daily zeros\n",
    "\n",
    "arrows =   dict({\n",
    "            'saffron_arrows1':\n",
    "            dict({'color':'#FF7722','angle':140,\n",
    "                  'xy': [[[.12,.05]],\n",
    "                         [[.12,.05]],\n",
    "                         [],\n",
    "                         []]}),\n",
    "            'saffron_arrows2':\n",
    "            dict({'color':'#FF7722','angle':20,\n",
    "                  'xy': [[[.77,.05]],\n",
    "                         [[.77,.05]],\n",
    "                         [],\n",
    "                         []]}),\n",
    "            'green_arrows':\n",
    "            dict({'color':'b','angle':120,\n",
    "                  'xy':[[],\n",
    "                        [],\n",
    "                        [[.125,.45],[.76,.32]],\n",
    "                        [[.125,.12],[.76,.75]]]})\n",
    "                })\n",
    "axes = plot_inverter(prod_p1,prod_p1_supp_data,11,start=2070,end=3030)\n",
    "annotate_arrows(axes,arrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7f85f99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_missing(df):\n",
    "    '''\n",
    "    Function to find missing timestamps ad it's corresponding missing inverters\n",
    "    find_missing(df: pandas.DataFrame)\n",
    "    \n",
    "    Returns pandas.DataFrame object containing the columns: \n",
    "        \"date_time\": missing Timestamps in \"date_time\" or\n",
    "                     present Timestamps with missing inverters.\n",
    "        \"source_key\" : list containing the \"source_key\" of the missing inverters\n",
    "                     for a given Timestamp''\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: production data of solar plant'''\n",
    "    \n",
    "    missing_data = pd.DataFrame({})\n",
    "    df = df.copy()\n",
    "    \n",
    "    date_time, dt_count = np.unique(df['date_time'].values,return_counts=True)\n",
    "    dt_range  = pd.date_range(start=date_time[0],\n",
    "                              end=date_time[-1],freq=\"15min\")\n",
    "    key = np.unique(df['source_key'].values)\n",
    "    \n",
    "    # find the datetimes that are not present in the datetimes of the df\n",
    "    missing_data['date_time'] = dt_range[np.isin(dt_range,date_time)==False]\n",
    "    source_key = [key for _ in range(len(missing_data['date_time']))]\n",
    "    missing_data['source_key'] = source_key\n",
    "    \n",
    "    # find which inverters are missing in the datetimes that have < 22 invs.   \n",
    "    dt_with_missing = date_time[dt_count < 22]\n",
    "    df_missing = df[np.isin(df['date_time'].values,dt_with_missing)]\n",
    "    for dt in dt_with_missing:\n",
    "        present_key= df_missing[df_missing['date_time']==dt]['source_key'].values\n",
    "        missing_key= key[np.isin(key,present_key)==False]\n",
    "        missing_data = pd.concat([missing_data, pd.DataFrame.from_dict({'date_time':dt,\n",
    "                            'source_key':missing_key})], ignore_index=True)\n",
    "#         missing_data = missing_data.concat({'date_time':dt,\n",
    "#                             'source_key':missing_key}, ignore_index=True)\n",
    "    missing_data.sort_values(by=['date_time'],ignore_index=True,inplace=True)\n",
    "    return missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5372dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_p1_missing_data = find_missing(prod_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "aa7df872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_night_missing(df, missing_data):\n",
    "    '''fill nighttime missing values into the DataFrame with 0 for dc_power,\n",
    "    ac_power and daily_yield and the total_yield missing is replaced with the\n",
    "    nearest previous value'''\n",
    "    \n",
    "    cols = ['date_time','source_key','dc_power','ac_power',\n",
    "            'daily_yield','total_yield'] \n",
    "    new_entries = pd.DataFrame([],columns=cols)\n",
    "    night_ix = pd.DatetimeIndex(missing_data['date_time'].values)\n",
    "    night_ix = night_ix.hour + night_ix.minute/60\n",
    "    night_ix = (night_ix < 6.00) | (night_ix > 18.5)\n",
    "    night_missing = missing_data[night_ix].explode('source_key')\n",
    "    for key in np.unique(night_missing['source_key'].values):\n",
    "        key_data = df[df['source_key']==key]\n",
    "        \n",
    "        #source_key missing during the night time = skmnt\n",
    "        skmnt = night_missing[night_missing['source_key']==key]['date_time'].values\n",
    "        skmnt = pd.DatetimeIndex(skmnt)\n",
    "        index = np.searchsorted( key_data['date_time'].values,skmnt)\n",
    "        nearest_total_yield = key_data.iloc[index-1]['total_yield']\n",
    "        source_key  = np.full(len(index),key, dtype=np.int8)\n",
    "        zeros = np.full(len(index),0.0, dtype=np.float32)\n",
    "        total_yield = np.full(len(index),nearest_total_yield, dtype=np.float32)\n",
    "        intermediate_df = pd.DataFrame( zip(skmnt, source_key,\n",
    "                                            zeros, zeros, zeros, total_yield),\n",
    "                                            columns = cols)\n",
    "#         new_entries = new_entries.append(intermediate_df,ignore_index=True )\n",
    "        new_entries = pd.concat([new_entries, intermediate_df], ignore_index=True)\n",
    "    new_entries['source_key'] = np.array(new_entries['source_key' ].values, np.int8)\n",
    "#     df = df.append(new_entries,ignore_index=True)\n",
    "    df = pd.concat([df, new_entries], ignore_index=True)\n",
    "    df.sort_values(by=['date_time'],ascending=True,ignore_index=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "def std_daily_yield(df):\n",
    "    'replace daily_yield with zero in hours between 18:30 and 6:00'\n",
    "    \n",
    "    date_time = pd.DatetimeIndex(df['date_time'].values)\n",
    "    date_time = date_time.hour + date_time.minute/60\n",
    "    ix = np.multiply((date_time <= 18.5), (date_time >= 6))\n",
    "    df['daily_yield'] = np.multiply(df['daily_yield'].values,ix)\n",
    "   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c8038baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_daytime_zeros(prod_df):\n",
    "    date_time = pd.DatetimeIndex(prod_df['date_time'].values)\n",
    "    date_time = date_time.hour + date_time.minute/60\n",
    "    day_ix = (date_time > 6.5) & (date_time < 18)\n",
    "    daytime_zeros = day_ix & (prod_df['ac_power'] <= 0)\n",
    "    return prod_df[daytime_zeros == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bc57c5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_p1 = remove_daytime_zeros(prod_p1)\n",
    "prod_p1 = fill_night_missing(prod_p1, prod_p1_missing_data)\n",
    "prod_p1 = std_daily_yield(prod_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ab875435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Therefore the first 4 values will be droped for this inverter\n",
    "\n",
    "prod_p1.drop(index=prod_p1[prod_p1['source_key']==7].iloc[:4].index,inplace=True)\n",
    "plt.figure(figsize=(12,3))\n",
    "sample = prod_p1[prod_p1['source_key']==7]['total_yield'].values\n",
    "plt.plot(range(len(sample)),sample)\n",
    "plt.show()\n",
    "sample[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "88cf0d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1ws = pd.read_csv('solar_data/Plant_1_Weather_Sensor_Data.csv')\n",
    "p1ws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4f1197aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1ws.info()\n",
    "p1ws.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "563b2bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_formats_ws(df):\n",
    "    if df.columns[0].isupper():\n",
    "        initial_size = getsizeof(df)\n",
    "        supp_data = dict({'plant_id':0,'sensors':[]})\n",
    "\n",
    "        # drop \"source_key\" , store original \"source_key\" in separate variable\n",
    "        supp_data['sensors'] = df['SOURCE_KEY'].values[0]\n",
    "        df.drop(columns=['SOURCE_KEY'], inplace=True)\n",
    "\n",
    "        # drop \"plant_id\" column and stores it's value in an external variable\"\n",
    "        supp_data['plant_id'] = df['PLANT_ID'].values[0]\n",
    "        df.drop(columns=['PLANT_ID'], inplace=True)\n",
    "\n",
    "        # change column names to lowercase, rename columns to shorter name\n",
    "        df.rename(columns={'DATE_TIME':'date_time','AMBIENT_TEMPERATURE': 'ambient_t',\n",
    "                           'MODULE_TEMPERATURE':'module_t', 'IRRADIATION':'irradiation'},\n",
    "                 inplace=True)\n",
    "\n",
    "        # change 'date_time' from string to pd.Timestamp\n",
    "        df['date_time'] = pd.DatetimeIndex(df['date_time'].values, dayfirst=True)\n",
    "        final_size = getsizeof(df) + getsizeof(supp_data)\n",
    "        print(f'Initial size: {initial_size/1e6:.2f} Mb')\n",
    "        print(f'Final size:    {final_size/1e6:.2f} Mb')\n",
    "        print(f\"Memory footprint reduction: {(initial_size - final_size)/initial_size*100:.2f}%\")\n",
    "    else:\n",
    "        raise ValueError(\"Formats allready optimized !\")\n",
    "    return df, supp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "efe1340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1ws, p1ws_supp_data = optimize_formats_ws(p1ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "bbc8e9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1ws.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0e103de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1ws_supp_data['sensors'],p1ws_supp_data['plant_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8adef915",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_p2 = pd.read_csv('solar_data/Plant_2_Generation_Data.csv')\n",
    "prod_p2[prod_p2['DC_POWER']>0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3da12a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_p2[prod_p2['DC_POWER']>0].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a1ed332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_p2, prod_p2_supp_data = optimize_formats(prod_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "cfe2641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_p2_missing_data = find_missing(prod_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e2f88010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the night-missing time-stamps with zeros for plant #2\n",
    "prod_p2 = fill_night_missing(prod_p2, prod_p2_missing_data)\n",
    "\n",
    "# Actualize the missing values after night-missing are filled\n",
    "prod_p2_missing_data = find_missing(prod_p2)\n",
    "\n",
    "# Standardize daily_yield to be zero when there is no longer production\n",
    "prod_p2 = std_daily_yield(prod_p2)\n",
    "\n",
    "# Remove zero or negative outliers from daytime\n",
    "prod_p2 = remove_daytime_zeros(prod_p2)\n",
    "\n",
    "# Actualize the missing values after night-missing are filled\n",
    "prod_p2_missing_data = find_missing(prod_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1e7e4aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_total_yield_anomalies(prod_df, variable):\n",
    "    new_ix = [] #list of the indexes without anomalies\n",
    "    for key in np.unique(prod_df['source_key'].values):\n",
    "        key_df = prod_df[prod_df['source_key']==key]\n",
    "        field = key_df[variable].values\n",
    "        if variable == 'daily_yield':\n",
    "                field = np.cumsum(field)\n",
    "        filter_ix = (field[1:]-field[0:-1]) < 0\n",
    "        present_outliers = np.any(filter_ix == True)\n",
    "        while present_outliers:\n",
    "            dt = pd.DatetimeIndex(key_df['date_time'].values)\n",
    "            field = key_df[variable].values\n",
    "            if variable == 'daily_yield':\n",
    "                field = np.cumsum(field)\n",
    "            filter_ix = np.hstack([[False],(field[1:]-field[0:-1]) < 0])\n",
    "            key_df = key_df[filter_ix == False]\n",
    "            field = key_df[variable].values\n",
    "            filter_ix = np.hstack([(field[1:]-field[0:-1]) < 0,[False]])\n",
    "            present_outliers = np.any(filter_ix == True)\n",
    "        new_ix += list(key_df.index)\n",
    "    new_ix = list(np.sort(new_ix))\n",
    "    init = len(prod_df.index)\n",
    "    end  = len(new_ix)\n",
    "    print(f'Initial Dataframe Length: {init}')\n",
    "    print(f'Final Dataframe length: {end}')\n",
    "    print(f'filtered out records: {init-end} ({(init-end)/init*100:.1f}%)')\n",
    "    return prod_df.loc[new_ix,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d0275d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_p2 = filter_total_yield_anomalies(prod_p2,'total_yield')\n",
    "prod_p1 = filter_total_yield_anomalies(prod_p1,'total_yield')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6c426597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_daily_yield_anomalies(prod_df):\n",
    "    df = prod_df.copy()\n",
    "    initial_size = len(df)\n",
    "    \n",
    "    def get_filter_values(sub_df):\n",
    "        dt = pd.DatetimeIndex(sub_df['date_time'].values)\n",
    "        daily_yield = sub_df['daily_yield'].values\n",
    "        delta = np.hstack([[0],daily_yield[1:]-daily_yield[0:-1]])\n",
    "        # \n",
    "        periods = np.hstack([ [1], (dt[1:] - dt[0:-1]).seconds/900])\n",
    "        delta = delta/periods\n",
    "        return (delta < 0) & (daily_yield != 0)\n",
    "        \n",
    "    for key in np.unique(df['source_key'].values):\n",
    "        key_df = df[df['source_key'] == key]\n",
    "        yield_filter = get_filter_values(key_df)\n",
    "        ix_to_drop = []\n",
    "        while np.any(yield_filter):\n",
    "            key_ix = key_df.index\n",
    "            bad_ix = [ key_ix[key_ix < ix][-1] for ix in  key_ix[yield_filter]]\n",
    "            ix_to_drop += bad_ix\n",
    "            key_df = key_df.drop(bad_ix)\n",
    "            yield_filter = get_filter_values(key_df)\n",
    "        df.drop(ix_to_drop, inplace=True)\n",
    "    print(f\"Dataset reduction: {(initial_size-len(df))/initial_size*100:0.2f}%\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "278312ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_p2 = filter_daily_yield_anomalies(prod_p2)\n",
    "prod_p1 = filter_daily_yield_anomalies(prod_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "318e5aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_p2_missing_data = find_missing(prod_p2)\n",
    "prod_p2 = fill_night_missing(prod_p2, prod_p2_missing_data)\n",
    "prod_p2_missing_data = find_missing(prod_p2)\n",
    "\n",
    "prod_p1_missing_data = find_missing(prod_p1)\n",
    "prod_p1 = fill_night_missing(prod_p1, prod_p1_missing_data)\n",
    "prod_p1_missing_data = find_missing(prod_p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d8d5eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "p2ws = pd.read_csv('solar_data/Plant_2_Weather_Sensor_Data.csv')\n",
    "\n",
    "p2ws, p2ws_supp_data = optimize_formats_ws(p2ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "df3ac141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization of the relationship between power production, temperature, and irradiation\n",
    "\n",
    "def power_vs_irr_vs_temp(sensor_df,prod_df,key):\n",
    "    key_df = pd.merge(left=sensor_df,right=prod_df,\n",
    "                      how='inner',on='date_time')\n",
    "    key_df = key_df[key_df['source_key']==key]\n",
    "    date_time = pd.DatetimeIndex(key_df['date_time'].values)\n",
    "    date_time = date_time.hour + date_time.minute/60\n",
    "    day_ix = (date_time >= 6) & (date_time <= 18.5)\n",
    "    key_df = key_df[day_ix]\n",
    "    key_df.drop(columns=key_df.columns.difference(['date_time','ambient_t',\n",
    "                    'module_t','irradiation','ac_power']),inplace=True)\n",
    "    key_df = key_df.sort_values(by=['irradiation'])\n",
    "    fig = plt.figure(figsize=(16,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    ax = plt.gca()\n",
    "    color_index = np.unique(key_df['irradiation'].values/(1.1*key_df['irradiation'].max())+0.1)\n",
    "    colors = plt.cm.Greens(X=color_index)[-1::-1]\n",
    "    ax.scatter(key_df['ambient_t'],key_df['ac_power'],color= colors,alpha=1)\n",
    "    ax.set_title(\"AC Power Vs Ambient Temperature Plant #1\")\n",
    "    ax.set_xlabel(\"Ambient Temperature\")\n",
    "    ax.set_ylabel(\"AC Power (Kw)\",labelpad=30)\n",
    "    ax.yaxis.get_label().set_rotation(-90)\n",
    "    plt.colorbar( plt.cm.ScalarMappable(norm=Normalize(vmax=0.0,vmin=1.2),\n",
    "                        cmap='Greens_r'), ticks=np.arange(0,1.21,0.2),label='Irradiation (W / $m^2$)')\n",
    "\n",
    "    fig.add_subplot(1,2,2,projection='3d')\n",
    "    ax = plt.gca()\n",
    "    ax.scatter(key_df['ambient_t'],key_df['irradiation'].values,key_df['ac_power'])\n",
    "    ax.view_init(elev=30,azim=-140)\n",
    "    ax.set_xlabel('Ambient Temperature (C)')\n",
    "    ax.set_ylabel('Irradiation (W / $m^2$)')\n",
    "    ax.set_zlabel('AC Power (Kw)')\n",
    "    ax.zaxis.get_label().set_rotation(1)\n",
    "    ax.set_title('AC Power Vs abient_t & Irradiation Plant #1 inv #1 ')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "af893732",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_vs_irr_vs_temp(p1ws,prod_p1,21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "79d0481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_features(prod_df,sensor_df,key_n):\n",
    "    df = prod_df.copy()[prod_df['source_key']==key_n]\n",
    "    df = pd.merge(left=sensor_df,right=df,how='inner',on='date_time')\n",
    "    hours = pd.DatetimeIndex(df['date_time'].values)\n",
    "    hours = hours.hour + hours.minute/60\n",
    "    df = df[(hours >= 6) & (hours <= 18.5)]\n",
    "    df.rename(columns={'irradiation':'Gir','ac_power':'pac', \n",
    "                       'ambient_t': 'Ta'}, inplace=True)\n",
    "    df = df[['date_time','source_key','Gir','Ta','pac']]\n",
    "    df['hours'] = hours[(hours >= 6) & (hours <= 18.5)]\n",
    "    df['Gir^3'] = np.power(df['Gir'],3)\n",
    "    df['Gir^2'] = np.power(df['Gir'],2)\n",
    "    df['Ta^2']  = np.power(df['Ta'],2)\n",
    "    df['Gir^2.Ta'] = np.multiply(df['Gir^2'],df['Ta'])\n",
    "    df['Gir.Ta^2'] = np.multiply(df['Gir'],df['Ta^2'])\n",
    "    df['Gir.Ta'] =  np.multiply(df['Gir'],df['Ta'])\n",
    "    df = df[['date_time','hours','Gir^3','Gir^2','Gir^2.Ta','Gir.Ta^2',\n",
    "                     'Gir.Ta','Gir','pac']]\n",
    "                     \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5a5eb678",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_p1_new_data = create_new_features(prod_p1,p1ws,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d119e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df):\n",
    "    X = df.drop(columns=['hours','pac'])\n",
    "    pac = df['pac'].values\n",
    "    train_size = int(0.8*len(pac))\n",
    "    X_train ,  X_test  =   X.iloc[:train_size,:],   X.iloc[train_size:,:];\n",
    "    pac_train, pac_test= pac[:train_size],   pac[train_size:];\n",
    "    #filter out pac=0\n",
    "    ix = pac_test > 0\n",
    "    pac_test = pac_test[ix]\n",
    "    X_test = X_test[ix]\n",
    "    return X_train, X_test, pac_train, pac_test;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "98813197",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, pac_train, pac_test = split_data(prod_p1_new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "385a474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_regressor(X_train, pac_train):\n",
    "    X_train = X_train.drop(columns=['date_time']).values\n",
    "    models = [\n",
    "            ('Linear', LinearRegression(fit_intercept =False, n_jobs=-1)),\n",
    "            ('Ridge', Ridge(fit_intercept =False, solver= 'lsqr', random_state=1973)),\n",
    "            ('DTree', DecisionTreeRegressor(random_state=1973)),\n",
    "            ('RForest', RandomForestRegressor(random_state=1973, n_jobs=-1)),\n",
    "            ('KNReg', KNeighborsRegressor(n_neighbors=5,n_jobs=-1)),\n",
    "            ]\n",
    "    scores = dict({})\n",
    "    ts_split = TimeSeriesSplit(n_splits=6)\n",
    "    for name,model in models:\n",
    "        folds = ts_split.split(X_train, pac_train)\n",
    "        regressor = clone(model)\n",
    "        cv_scores = cross_validate(regressor, X_train, pac_train,\n",
    "                                   cv=folds, scoring='neg_mean_squared_error')\n",
    "        scores[name.rjust(10,\" \")] = np.mean(cv_scores['test_score'])\n",
    "    scores = pd.DataFrame(scores.items(),columns=['regressor','Neg_MSE'])\n",
    "    scores = scores.sort_values(by=['Neg_MSE'],ascending=False,ignore_index=True)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "2b14f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best_regressor(X_train, pac_train )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "7dc566c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_models(prod_df,sensor_df):\n",
    "    models = []\n",
    "    predictions_df = pd.DataFrame({})\n",
    "    rmse = []\n",
    "    def get_final_model(X, X_t, pac, pac_t):\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, pac)\n",
    "        pac_pred = model.predict(X_t).flatten()\n",
    "        return model, pac_pred, mean_squared_error(pac_t, pac_pred)\n",
    "    \n",
    "    for key in np.unique(prod_df['source_key'].values):\n",
    "        new_features = create_new_features(prod_df,sensor_df,key)\n",
    "        X_train, X_test, pac_train, pac_test = split_data(new_features)\n",
    "        date_time = X_test['date_time'].values\n",
    "        X_train = X_train.drop(columns=['date_time']).values\n",
    "        X_test  = X_test.drop(columns=['date_time']).values\n",
    "        model, pac_predicted, mse = get_final_model(X_train, X_test,\n",
    "                                                    pac_train, pac_test)\n",
    "        models += [model]\n",
    "        rmse += [np.sqrt(mse)]\n",
    "        r2score = int(100*r2_score(pac_test,pac_predicted))\n",
    "        residuals = (pac_predicted-pac_test)\n",
    "        str1 = f'#{str(key).rjust(2)}   --->   '\n",
    "        str2 = f' rmse: {int(np.sqrt(mse))}'\n",
    "        str3 = f',      R^2 (Determination coeff.): {r2score}%'\n",
    "        print( str1 + str2 +str3)\n",
    "        predictions_df = pd.concat([predictions_df, pd.DataFrame.from_dict({\n",
    "              'date_time':date_time,'source_key':np.full(len(residuals),key),\n",
    "              'pac':pac_test, 'pac_predicted':pac_predicted,\n",
    "              'residuals':residuals})])\n",
    "    predictions_df.sort_values(by='date_time', ignore_index=True, inplace=True)\n",
    "    return models, predictions_df, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d9466e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "( plant1_models, plant1_predictions,\n",
    "plant1_rmses ) = construct_models(prod_p1, p1ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "fd093651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot AC_Power test-set and predicted AC_Power for this set.\n",
    "def plot_sample(predictions_df,key):\n",
    "    key_df = predictions_df[predictions_df['source_key']==key]\n",
    "    plt.figure(figsize=(16,3))\n",
    "    plt.plot(key_df['date_time'],key_df['pac'],label='Pac',alpha=0.7)\n",
    "    plt.plot(key_df['date_time'],key_df['pac_predicted'], alpha=0.7,label=\"prediction\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(f\"prediction and actual AC_POWER for inverter #{key}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "09c8101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample(plant1_predictions, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3f329737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_residuals(prediction_df,figsize=(16,16),set_title=True):\n",
    "    keys = np.unique(prediction_df['source_key'].values)\n",
    "    cols = 4\n",
    "    rows = len(keys)//cols + 1*(len(keys)>0)\n",
    "    if rows == 1:\n",
    "        cols = len(keys)\n",
    "    fig ,axes = plt.subplots(nrows=rows,ncols=cols,figsize=figsize)\n",
    "    if type(axes) == np.ndarray:\n",
    "        axes = axes.ravel()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "    for key,ax in zip(keys, axes[:len(keys)]):\n",
    "        key_df =  prediction_df[prediction_df['source_key']==key]\n",
    "        residuals = key_df['residuals'].values\n",
    "        plt.subplot(rows,cols,key+1)\n",
    "        ax.hist(residuals, bins=50, density=True,color='navy',\n",
    "                label = 'Residuals')\n",
    "        if set_title == True:\n",
    "            title = f\"Residuals distribution for Inv #{key}\"\n",
    "            ax.set_title(title)\n",
    "        xmin,xmax = ax.get_xlim()\n",
    "        ymin,ymax = ax.get_ylim()\n",
    "        xticks=np.linspace(xmin,xmax,5)\n",
    "        labels=[f'{int(x)}' for x in xticks]\n",
    "        plt.xticks(ticks=xticks,labels=labels)\n",
    "        # make room for the legends\n",
    "        plt.gca().set_xlim((xmin,xmax + (xmax-xmin)*0.65))\n",
    "        plt.vlines(xmax,ymin,ymax,color='k')\n",
    "        plt.gca().set_ylim((ymin,ymax))\n",
    "        \n",
    "        #draw legends\n",
    "        n = len(residuals)\n",
    "        mu = residuals.mean()\n",
    "        m2 = np.sum(np.power(residuals-mu,2))/n\n",
    "        m3 = np.sum(np.power(residuals-mu,3))/n\n",
    "        m4 = np.sum(np.power(residuals-mu,4))/n\n",
    "        mu = residuals.mean()\n",
    "        sigma  = residuals.std()\n",
    "        kurt = m4/(m2**2)\n",
    "        skew = m3/(m2**1.5)\n",
    "        rmse = int(np.sqrt(np.sum(np.power(residuals,2))/n))\n",
    "        r2score = int(100*r2_score(key_df['pac'],key_df['pac_predicted']))\n",
    "        legends = [ \n",
    "            f'     $\\mu$ :' + f'{mu:0.1f}'.rjust(6,\" \"),\n",
    "            f'     $\\sigma$ :' +  f'{sigma:0.1f}'.rjust(6,\" \"),\n",
    "            f'   kurt:' + f'{kurt:0.1f}'.rjust(6,\" \"),\n",
    "            f'skew:' + f'{skew:0.1f}'.rjust(6,\" \"),\n",
    "            f' rmse:' + f'{rmse}'.rjust(6,\" \"),\n",
    "            f'  R^2 :' + f'{r2score}%'.rjust(6,\" \"),]\n",
    "        for i,legend in enumerate(legends):\n",
    "            plt.text(xmax + (xmax-xmin)*0.05,\n",
    "                     ymax-(ymax-ymin)*(0.11*(i+1)), legend,color='navy')\n",
    "            \n",
    "    # do not show empty plots\n",
    "    for i in range(rows*cols)[len(keys):]:\n",
    "        axes[i].axis('off')\n",
    "    fig.subplots_adjust(hspace=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "203828ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_residuals(plant1_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "6ebedd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plant 2\n",
    "\n",
    "power_vs_irr_vs_temp(p2ws,prod_p2,21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "dd2fbe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_p2_new_data = create_new_features(prod_p2, p2ws, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "5f80ece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "( plant2_models, plant2_predictions,\n",
    "plant2_rmses ) = construct_models(prod_p2, p2ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "63da3c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample(plant2_predictions,4)\n",
    "plot_all_residuals(plant2_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41f3f26",
   "metadata": {},
   "source": [
    "# Business Plan\n",
    "\n",
    "### Executive Summary:\n",
    "\n",
    " \n",
    "\n",
    "Our company offers an innovative AI-powered chatbot solution designed to optimize energy usage, facilitate decarbonization efforts, and enhance sustainability. We target electrical companies, including prominent players like PGNE and their international counterparts, as our primary customers. Additionally, defense companies and the airplane industry, which are transitioning to alternate energy resources like hydrogen power, present potential customer segments.\n",
    "\n",
    " \n",
    "\n",
    "Our solution leverages predictive models, optimization algorithms, and real-time data analysis to provide actionable insights and recommendations. By accurately predicting energy demand, optimizing renewable energy utilization, suggesting optimal locations for EV charging stations, and addressing the need for longer-term backup capacity, our solution enables customers to reduce carbon emissions, improve energy efficiency, and meet sustainability goals.\n",
    "\n",
    " \n",
    "\n",
    "The market for energy optimization and decarbonization solutions is rapidly growing, driven by the increasing global focus on mitigating climate change and transitioning to clean energy. Electrical companies, defense organizations, and the airplane industry are actively seeking innovative technologies to enhance their energy efficiency and reduce their environmental impact. Our solution caters to these market segments by offering a comprehensive AI-powered platform that aligns with their sustainability objectives.\n",
    "\n",
    " \n",
    "\n",
    "### Market Analysis:\n",
    "\n",
    " \n",
    "\n",
    "The market for energy optimization and decarbonization solutions is experiencing significant growth due to rising environmental concerns and increased regulatory pressures. Electrical companies are under increasing pressure to reduce their carbon footprint and transition to clean energy sources. Defense organizations and the airplane industry are also embracing sustainability initiatives and seeking solutions that optimize their energy usage while transitioning to alternative energy resources.\n",
    "\n",
    " \n",
    "\n",
    "Our solution aligns perfectly with the market needs as it offers a comprehensive platform that addresses the specific challenges faced by these industries. By providing accurate predictions of energy demand, optimizing renewable energy utilization, and suggesting optimal EV charging station locations, our solution enables customers to make informed decisions that reduce emissions, improve efficiency, and align with their sustainability goals.\n",
    "\n",
    " \n",
    "\n",
    "### Product Differentiation:\n",
    "\n",
    " \n",
    "\n",
    "Our solution stands out from existing offerings through its advanced AI capabilities, customization options, and real-time optimization features. The chatbot's integration with predictive models and optimization algorithms provides valuable insights tailored to customers' specific needs. The ability to predict energy demand, suggest optimal renewable energy utilization, optimize EV charging station locations, and address backup capacity requirements sets us apart from competitors. Moreover, our solution's user-friendly interface and interactive nature enhance customer experience and ease of adoption.\n",
    "\n",
    " \n",
    "\n",
    "The customization options offered by our solution allow customers to tailor the chatbot's recommendations and insights to their specific requirements. This flexibility ensures that the solution can adapt to different energy infrastructures and unique operational demands, making it a highly desirable choice for electrical companies, defense organizations, and the airplane industry.\n",
    "\n",
    " \n",
    "\n",
    "### Customer Acquisition and Benefits:\n",
    "\n",
    " \n",
    "\n",
    "Our strategy for customer acquisition will focus on targeted marketing campaigns, partnerships with industry leaders, and demonstrations of our solution's effectiveness. Electrical companies will be motivated to adopt our solution to achieve their decarbonization goals, enhance energy efficiency, and reduce operational costs. Defense companies and the airplane industry can benefit from our solution's ability to optimize their transition to alternate energy resources like hydrogen power. Our product's desirability lies in its ability to offer real-time, data-driven recommendations that optimize energy usage, improve sustainability, and ensure regulatory compliance.\n",
    "\n",
    " \n",
    "\n",
    "Customers adopting our solution can expect numerous benefits. Firstly, they will experience improved energy efficiency, leading to reduced operational costs and increased profitability. Secondly, our solution helps customers meet their sustainability goals by reducing carbon emissions and reliance on fossil fuels. Thirdly, the real-time optimization capabilities of our solution ensure that customers can adapt quickly to changing energy demand patterns, resulting in improved grid stability and flexibility.\n",
    "\n",
    " \n",
    "\n",
    "### Revenue Model:\n",
    "\n",
    " \n",
    "\n",
    "Our revenue model will be based on a subscription-based pricing structure, where customers pay a recurring fee to access and utilize our AI-powered chatbot and optimization features. The subscription fee will be tailored to the size and requirements of each customer, ensuring scalability and affordability. Additional revenue streams may include customization services, data analytics insights, and value-added features.\n",
    "\n",
    " \n",
    "\n",
    "To ensure customer satisfaction and long-term relationships, we will provide ongoing customer support and updates, ensuring that our solution continuously evolves to meet the changing needs of our customers.\n",
    "\n",
    " \n",
    "\n",
    "### Financial Projections:\n",
    "\n",
    " \n",
    "\n",
    "Our financial projections are based on anticipated customer acquisition rates, average subscription fees, and cost analysis. We expect a steady increase in revenue as we acquire customers and expand our market presence. Investments will be allocated to research and development, marketing, talent acquisition, and infrastructure development. Detailed financial projections, including revenue, expenses, and profitability, will be provided in the attached financial statement.\n",
    "\n",
    " \n",
    "\n",
    "In conclusion, our AI-powered energy optimization solution holds significant viability, feasibility, and desirability in the market. The combination of advanced AI capabilities, real-time optimization, and customization options positions us as a leader in the energy decarbonization landscape. By targeting electrical companies, defense organizations, and the airplane industry, we are poised to capture a significant market share and contribute to global sustainability goals. With a strong business plan, solid financial projections, and a focus on customer satisfaction, our company is well-positioned for success in the rapidly evolving energy optimization market.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ff9821",
   "metadata": {},
   "source": [
    "# Data Privacy and Ethics\n",
    "Data ethics and privacy are of paramount importance in our AI-powered energy optimization solution. We recognize that the datasets we use, such as the energy consumption data from PJM and the solar plant data from India, contain sensitive information and must be handled with utmost care.\n",
    "\n",
    " \n",
    "\n",
    "To ensure data ethics, we strictly adhere to privacy regulations and industry best practices. We anonymize and encrypt personal data, removing any personally identifiable information from the datasets. We also implement strong security measures to protect the data from unauthorized access or breaches.\n",
    "\n",
    " \n",
    "\n",
    "Furthermore, we obtain proper consent and permissions when collecting and using the data. We ensure transparency by clearly communicating the purpose of data collection and obtaining informed consent from individuals or organizations involved. Our data collection and usage processes comply with applicable data protection laws and regulations, safeguarding the privacy of individuals and organizations involved in the datasets.\n",
    "\n",
    " \n",
    "\n",
    "We also prioritize data minimization and data retention policies. We only collect and retain the data necessary for the functioning of our solution, and we promptly discard any unnecessary or outdated information. This approach helps reduce the risk of data misuse and ensures compliance with data protection principles.\n",
    "\n",
    " \n",
    "\n",
    "In summary, data ethics and privacy are foundational principles in our solution. We are committed to handling data responsibly, complying with privacy regulations, and implementing robust security measures to protect the privacy and confidentiality of the datasets we use. By upholding these principles, we maintain the trust of our customers and prioritize the ethical use of data in our AI-powered energy optimization solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fede19af",
   "metadata": {},
   "source": [
    "# Challenges\n",
    "Another challenge we faced was finding datasets that encompassed all the necessary variables and factors required for our solution. The availability and accessibility of relevant and comprehensive datasets can be a challenge, especially when aiming to incorporate various parameters such as energy consumption patterns, renewable energy availability, and geographical data. We dedicated substantial effort to sourcing and curating datasets that would provide the foundation for accurate predictions and optimizations.\n",
    "\n",
    " \n",
    "\n",
    "Additionally, we encountered a hurdle in obtaining access to an API that would provide us with the real-time data needed to identify optimal EV charging station positions. Without direct access to the API, we had to rely on alternative approaches and approximations to generate recommendations for optimal charging station locations. Despite this limitation, we leveraged available data and applied our expertise to design an AI algorithm that optimized EV charging station placements based on factors like traffic patterns and population density.\n",
    "\n",
    " \n",
    "\n",
    "Despite these challenges, we persevered and developed a viable solution that addresses energy optimization, decarbonization, and sustainability. Our commitment to problem-solving, creativity, and resourcefulness allowed us to overcome these obstacles and deliver a valuable solution during the hackathon.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd1887d",
   "metadata": {},
   "source": [
    "# Future Directions\n",
    "In terms of future directions and next steps for the project, there are several key areas to focus on. First, implementing the chatbot and conducting benchmarking against other AI models like Claude and ChatGPT would be crucial. This evaluation will help assess the performance and effectiveness of our chatbot's responses, ensuring that it meets or exceeds the standards set by existing AI models in terms of accuracy and quality.\n",
    "\n",
    " \n",
    "\n",
    "Expanding the available data to make the models more robust and applicable to larger areas is another important step. By acquiring additional datasets from different regions, we can enhance the predictive capabilities of our models and ensure their viability in various geographical contexts.\n",
    "\n",
    " \n",
    "\n",
    "Implementing optimal EV charger locations based on traffic patterns, population density, and other relevant factors is an essential aspect of the project. By incorporating this feature, we enable electrical companies to strategically plan the placement of charging infrastructure, further promoting the adoption of electric vehicles and supporting the transition to cleaner transportation.\n",
    "\n",
    " \n",
    "\n",
    "Refining our time series models by utilizing transformers instead of neural networks can enhance the accuracy and ability to establish long-term patterns. Transformers have shown promise in effectively capturing temporal dependencies and long-term trends, making them suitable for time series analysis tasks.\n",
    "\n",
    " \n",
    "\n",
    "Designing an architecture that integrates all the models into a cohesive and interactive chatbot is another important step. This architecture should facilitate seamless interaction with users, provide accurate predictions and recommendations, and ensure a smooth user experience.\n",
    "\n",
    " \n",
    "\n",
    "Lastly, benchmarking the current models with additional datasets to assess their accuracy and performance is essential. By comparing the outputs of our models against diverse datasets, we can gain insights into their effectiveness and identify areas for improvement.\n",
    "\n",
    " \n",
    "\n",
    "By focusing on these future directions and next steps, we can further refine and enhance the capabilities of our solution, ensuring its effectiveness, accuracy, and scalability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
